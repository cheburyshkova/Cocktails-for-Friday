{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8OFTUJRDAEXUNXP+QPv8P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheburyshkova/declarationanalytics/blob/main/declarationanalytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обработка данных"
      ],
      "metadata": {
        "id": "XS6RV5rKSQTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "id": "GNqt5JyIPZKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2"
      ],
      "metadata": {
        "id": "Kfbu4V91NMfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import List\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "0BuO3Qv42EO8"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"product_data.csv\")\n",
        "\n",
        "#data.head()"
      ],
      "metadata": {
        "id": "BGDWC9D7U2Nx"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize_row(row: pd.Series) -> pd.Series:\n",
        "\n",
        "    tokens_manufacturer_address = word_tokenize(row['Manufacturer Address'])\n",
        "\n",
        "    tokens_generic_product_name = word_tokenize(row['Generic Product Name']) if 'Generic Product Name' in row else []\n",
        "\n",
        "    return pd.Series([tokens_generic_product_name, tokens_manufacturer_address])\n",
        "\n",
        "#data[['Tokens_Generic_Product_Name', 'Tokens_Manufacturer_Address']] = data.apply(tokenize_row, axis=1)\n",
        "#data[['Generic Product Name', 'Tokens_Generic_Product_Name', 'Manufacturer Address', 'Tokens_Manufacturer_Address']].head()"
      ],
      "metadata": {
        "id": "6YlosoED8XkB"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "countries_list = [\n",
        "    'япония', 'китай', 'беларусь', 'соединенные штаты', 'иран', 'турция',\n",
        "    'россия', 'германия', 'франция', 'италия', 'испания',\n",
        "    'соединенное королевство великобритании', 'греция', 'бельгия',\n",
        "    'вьетнам', 'бразилия', 'гонконг', 'доминиканская республика',\n",
        "    'индонезия', 'ирландия', 'финляндия', 'корея', 'узбекистан',\n",
        "    'тайвань', 'пакистан', 'польша', 'соединенное королевство', 'австрия', 'чили', 'швейцария,', 'швеция', 'португалия', 'нидерланды',\n",
        "    'мексика', 'китайская народная республика'\n",
        "]"
      ],
      "metadata": {
        "id": "004m-HSOZ9jF"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_country_from_address(address: str, countries_list: List[str]) -> str:\n",
        "\n",
        "    for country in countries_list:\n",
        "        if country.lower() in address.lower():\n",
        "            return country\n",
        "    return \"неизвестная страна\"\n",
        "\n",
        "data['Country'] = data['Manufacturer Address'].apply(lambda x: extract_country_from_address(x, countries_list))\n",
        "\n",
        "data[['Manufacturer Address', 'Country']].head()"
      ],
      "metadata": {
        "id": "BJLFMcQwSP2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str) -> str:\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'[a-zA-Z0-9]', '', text)\n",
        "    return text\n",
        "\n",
        "data['Cleaned_Product_Name'] = data['Generic Product Name'].apply(clean_text)\n",
        "\n",
        "#data[['Generic Product Name', 'Cleaned_Product_Name']].head()"
      ],
      "metadata": {
        "id": "Fw78NG_HsNHg"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph = MorphAnalyzer()\n",
        "\n",
        "def lemmatize_text(text: str, morph_analyzer: MorphAnalyzer) -> str:\n",
        "\n",
        "    try:\n",
        "        tokens = word_tokenize(text)\n",
        "        lemmatized_tokens = [morph_analyzer.parse(word)[0].normal_form for word in tokens]\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при лемматизации: {e}\")\n",
        "        return text\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "data['Lemmatized'] = data['Cleaned_Product_Name'].apply(lambda x: lemmatize_text(x, morph))\n",
        "\n",
        "#data[['Lemmatized']].head()"
      ],
      "metadata": {
        "id": "ggC7ujvCLWEq"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_total_word_frequency(data: pd.DataFrame, column_name: str, top_n: int) -> pd.DataFrame:\n",
        "\n",
        "    try:\n",
        "        big_text = ' '.join(data[column_name].astype(str))\n",
        "        tokens = word_tokenize(big_text)\n",
        "        word_freq = Counter(tokens)\n",
        "        freq_df = pd.DataFrame(word_freq.items(), columns=['Word', 'Frequency']).sort_values(by='Frequency', ascending=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при вычислении частоты слов: {e}\")\n",
        "        return pd.DataFrame(columns=['Word', 'Frequency'])\n",
        "    return freq_df.head(top_n)\n",
        "\n",
        "total_word_frequency = calculate_total_word_frequency(data, 'Lemmatized', 10)\n",
        "\n",
        "top_words = total_word_frequency['Word'].tolist()\n",
        "\n",
        "print(top_words)"
      ],
      "metadata": {
        "id": "jUzZSIYUD9sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_adjectives(words: List[str]) -> List[str]:\n",
        "\n",
        "    morph = pymorphy2.MorphAnalyzer()\n",
        "    adjectives = [word for word in words if 'ADJF' in morph.parse(word)[0].tag or 'ADJS' in morph.parse(word)[0].tag]\n",
        "    return adjectives\n",
        "\n",
        "adjectives_list = extract_adjectives(top_words)\n",
        "\n",
        "print(adjectives_list)"
      ],
      "metadata": {
        "id": "cuV1zB-wNtWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_prepositions_and_conjunctions(words: List[str]) -> List[str]:\n",
        "\n",
        "    morph = pymorphy2.MorphAnalyzer()\n",
        "    prepositions_and_conjunctions = [word for word in words if 'PREP' in morph.parse(word)[0].tag or 'CONJ' in morph.parse(word)[0].tag]\n",
        "    return prepositions_and_conjunctions\n",
        "\n",
        "prepositions_and_conjunctions_list = extract_prepositions_and_conjunctions(top_words)\n",
        "\n",
        "print(prepositions_and_conjunctions_list)"
      ],
      "metadata": {
        "id": "ROgf4fTjyuBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_verbs(words: List[str]) -> List[str]:\n",
        "\n",
        "    morph = pymorphy2.MorphAnalyzer()\n",
        "    verbs = [word for word in words if 'VERB' in morph.parse(word)[0].tag or 'INFN' in morph.parse(word)[0].tag]\n",
        "    return verbs\n",
        "\n",
        "verbs_list = extract_verbs(top_words)\n",
        "\n",
        "print(verbs_list)"
      ],
      "metadata": {
        "id": "2ZXfNIK13D5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(text: str, stop_words: list) -> str:\n",
        "\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "stop_words_list = ['в', 'из', 'с', 'для','и', 'до', 'на', 'или', 'за', 'от', 'тот', 'упаковка', 'марка','артикул', 'модель',  'общий', 'свежий', 'искусственный', 'предмет', 'си', 'набор', 'год', 'номинальный', 'г', 'они', 'назначение', 'кожа', 'старый', 'материал', 'элемент', 'го', 'он', 'к', 'число', 'в_', 'отдельный', 'успокаивать', 'верхний', 'мужской', 'летний', 'то', 'под', 'работать', 'сочетание', 'конкест', 'гдм', 'пикет', 'ла', 'корк', 'номер', 'месяц', 'индекс', 'лежание', 'товарный', 'знак', 'транспортный', 'промышленность', 'технологический', 'промышленный', 'верхом', 'ирландский', 'вест', 'каск', 'сочетание', 'мпа', 'марк', 'расчётный', 'тип', 'не','второй', 'происхождение', 'слой', 'готовый', 'пищевой', 'происхождение', 'ребёнок', 'женщина', 'сойка', 'поставлять', 'отдельный','белый', 'световой', 'серия', 'тёплый', 'зелёный', 'средство', 'уход',\n",
        "'экстракт', 'эссенция', 'концентрат', 'стать', 'отдельный', 'оборудование','синтетический', 'маркировка', 'покрытие', 'скорость', 'гвладивосток', 'ултигровый', 'край', 'цель', 'натуральный', 'собственный', 'изготовление', 'февраль', 'выпуск', 'обрабатывать', 'центр', 'компонент', 'мужчина', 'послепродажный', 'изделие', 'отдельный','торговый', 'товарный', 'знак', 'часть', 'качество', 'взрослый', 'дорожный', 'ип', 'л', 'смешанный']\n",
        "\n",
        "data['Product_Name'] = data['Lemmatized'].apply(lambda x: remove_stop_words(x, stop_words_list))\n",
        "\n",
        "#data[['Product_Name']].head()"
      ],
      "metadata": {
        "id": "snzlnyMytKqD"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keep_first_five_words(text):\n",
        "\n",
        "    words = text.split()\n",
        "    return ' '.join(words[:5])\n",
        "\n",
        "data['Products'] = data['Product_Name'].apply(keep_first_five_words)\n",
        "\n",
        "#data[['Products']].head()"
      ],
      "metadata": {
        "id": "ffQzMF1MFLwM"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = data[['Products', 'Country']]\n",
        "dataset.to_csv('dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "5l6MdV_M-0eb"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('dataset.csv', encoding='utf-8')"
      ],
      "metadata": {
        "id": "sWtc-PrXi7Zm"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['Products'], inplace=True)"
      ],
      "metadata": {
        "id": "xovCUMZhmz-M"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df['Products'])"
      ],
      "metadata": {
        "id": "JOgYVXF3i7o7"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters = 7\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=143)\n",
        "df['Cluster'] = kmeans.fit_predict(X)"
      ],
      "metadata": {
        "id": "tszhUo_rlhl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "terms = vectorizer.get_feature_names_out()\n",
        "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "top_words = []\n",
        "for i in range(num_clusters):\n",
        "    cluster_words = [terms[ind] for ind in order_centroids[i, :5]]\n",
        "    top_words.append(cluster_words)\n",
        "\n",
        "\n",
        "for i, words in enumerate(top_words):\n",
        "    print(f\"Cluster {i+1}: {', '.join(words)}\")"
      ],
      "metadata": {
        "id": "WRp92-MznAQK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}